{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"JAVA_HOME\"] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = 'pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"SPARK_HOME\"] = '/home/rustem/Programs/spark-2.2.0-bin-hadoop2.7/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(os.environ['SPARK_HOME']+\"/python/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(os.environ['SPARK_HOME']+\"/python/lib/py4j-0.10.4-src.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import py4j\n",
    "from pyspark import SparkContext, SparkConf, SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = (SparkConf().setMaster(\"local[8]\")\n",
    "        .setAppName(\"ML demo\")\n",
    "        .set(\"spark.executor.memory\", \"2g\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlcontext.read.format(\n",
    "    'com.databricks.spark.csv').options(\n",
    "    header='true').load('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(PassengerId=u'1', Survived=u'0', Pclass=u'3', Name=u'Braund, Mr. Owen Harris', Sex=u'male', Age=u'22', SibSp=u'1', Parch=u'0', Ticket=u'A/5 21171', Fare=u'7.25', Cabin=None, Embarked=u'S'),\n",
       " Row(PassengerId=u'2', Survived=u'1', Pclass=u'1', Name=u'Cumings, Mrs. John Bradley (Florence Briggs Thayer)', Sex=u'female', Age=u'38', SibSp=u'1', Parch=u'0', Ticket=u'PC 17599', Fare=u'71.2833', Cabin=u'C85', Embarked=u'C'),\n",
       " Row(PassengerId=u'3', Survived=u'1', Pclass=u'3', Name=u'Heikkinen, Miss. Laina', Sex=u'female', Age=u'26', SibSp=u'0', Parch=u'0', Ticket=u'STON/O2. 3101282', Fare=u'7.925', Cabin=None, Embarked=u'S')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# добавить 5 новых фичей\n",
    "# 3 фичи высчитываются из имеющихся\n",
    "# хотя бы одна использует udf\n",
    "\n",
    "# попробовать 3 новых модели\n",
    "\n",
    "# f1 меру"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "## Add new features\n",
    "### Old features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlcontext.read.format(\n",
    "    'com.databricks.spark.csv').options(\n",
    "    header='true').load('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(EmbarkedVec=SparseVector(3, {2: 1.0})),\n",
       " Row(EmbarkedVec=SparseVector(3, {1: 1.0})),\n",
       " Row(EmbarkedVec=SparseVector(3, {0: 1.0})),\n",
       " Row(EmbarkedVec=SparseVector(3, {}))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import types\n",
    "\n",
    "def parse_age(str_age):\n",
    "    try:\n",
    "        return float(str_age)\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "def Embarked_transform(x):\n",
    "    if x != None:\n",
    "        return x\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "my_udf = udf(Embarked_transform, types.StringType())\n",
    "df = df.withColumn('Embarked', my_udf(df['Embarked']))\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"Embarked\", outputCol=\"EmbarkedIndex\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "encoder = OneHotEncoder(inputCol=\"EmbarkedIndex\", outputCol=\"EmbarkedVec\")\n",
    "df = encoder.transform(indexed)\n",
    "df.select(\"EmbarkedVec\").distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Feauture Miss/Mrs./Mr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(NamePrefixIndex=1),\n",
       " Row(NamePrefixIndex=3),\n",
       " Row(NamePrefixIndex=5),\n",
       " Row(NamePrefixIndex=4),\n",
       " Row(NamePrefixIndex=2)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Name_transform(x):\n",
    "    if x == None:\n",
    "        return 0\n",
    "    elif x.find(' Mr. ') != -1:\n",
    "        return 1\n",
    "    elif x.find(' Mrs. ') != -1:\n",
    "        return 2\n",
    "    elif x.find(' Miss. ') != -1:\n",
    "        return 3\n",
    "    elif x.find(' Ms. ') != -1:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "my_udf = udf(Name_transform, types.IntegerType())\n",
    "df = df.withColumn('NamePrefixIndex', my_udf(df['Name']))\n",
    "df.take(1)\n",
    "df.select('NamePrefixIndex').distinct().collect()\n",
    "encoder = OneHotEncoder(inputCol='NamePrefixIndex', outputCol='NamePrefixVec')\n",
    "df = encoder.transform(df)\n",
    "df.select(\"NamePrefixIndex\").distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Ticket number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Ticket_transform(x):\n",
    "    if x == None:\n",
    "        return -1\n",
    "    else:\n",
    "        try:\n",
    "            z = x[x.rfind(' ')+1:]\n",
    "            return int(z)\n",
    "        except:\n",
    "            return -1\n",
    "        \n",
    "my_udf = udf(Ticket_transform, types.IntegerType())\n",
    "df = df.withColumn('TicketNumber', my_udf(df['Ticket']))\n",
    "#df.select('TicketNumber','Ticket').distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Cabin letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CabinLetterVec=SparseVector(8, {4: 1.0})),\n",
       " Row(CabinLetterVec=SparseVector(8, {3: 1.0})),\n",
       " Row(CabinLetterVec=SparseVector(8, {1: 1.0})),\n",
       " Row(CabinLetterVec=SparseVector(8, {6: 1.0})),\n",
       " Row(CabinLetterVec=SparseVector(8, {5: 1.0})),\n",
       " Row(CabinLetterVec=SparseVector(8, {2: 1.0})),\n",
       " Row(CabinLetterVec=SparseVector(8, {0: 1.0})),\n",
       " Row(CabinLetterVec=SparseVector(8, {})),\n",
       " Row(CabinLetterVec=SparseVector(8, {7: 1.0}))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Cabin_transform(x):\n",
    "    if x == None:\n",
    "        return ' '\n",
    "    else:\n",
    "        return x[0]\n",
    "    \n",
    "        \n",
    "\n",
    "my_udf = udf(Cabin_transform, types.StringType())\n",
    "df = df.withColumn('CabinLetter', my_udf(df['Cabin']))\n",
    "stringIndexer = StringIndexer(inputCol=\"CabinLetter\", outputCol=\"CabinLetterIndex\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "encoder = OneHotEncoder(inputCol='CabinLetterIndex', outputCol='CabinLetterVec')\n",
    "df = encoder.transform(indexed)\n",
    "df.select(\"CabinLetterVec\").distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Cabin number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(PassengerId=u'1', Survived=u'0', Pclass=u'3', Name=u'Braund, Mr. Owen Harris', Sex=u'male', Age=u'22', SibSp=u'1', Parch=u'0', Ticket=u'A/5 21171', Fare=u'7.25', Cabin=None, Embarked=u'S', EmbarkedIndex=0.0, EmbarkedVec=SparseVector(3, {0: 1.0}), NamePrefixIndex=1, NamePrefixVec=SparseVector(5, {1: 1.0}), TicketNumber=21171, CabinLetter=u' ', CabinLetterIndex=0.0, CabinLetterVec=SparseVector(8, {0: 1.0}), CabinNum=u'-1'),\n",
       " Row(PassengerId=u'2', Survived=u'1', Pclass=u'1', Name=u'Cumings, Mrs. John Bradley (Florence Briggs Thayer)', Sex=u'female', Age=u'38', SibSp=u'1', Parch=u'0', Ticket=u'PC 17599', Fare=u'71.2833', Cabin=u'C85', Embarked=u'C', EmbarkedIndex=1.0, EmbarkedVec=SparseVector(3, {1: 1.0}), NamePrefixIndex=2, NamePrefixVec=SparseVector(5, {2: 1.0}), TicketNumber=17599, CabinLetter=u'C', CabinLetterIndex=1.0, CabinLetterVec=SparseVector(8, {1: 1.0}), CabinNum=u'85')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def CabinNum_transform(x):\n",
    "    try:\n",
    "        return int(x[2+x.rfind(' '):]) # because typical format is something like: 'A12' or 'B12 B14 B16'\n",
    "    except:\n",
    "        return -1\n",
    "        \n",
    "\n",
    "my_udf = udf(CabinNum_transform, types.StringType())\n",
    "df = df.withColumn('CabinNum', my_udf(df['Cabin']))\n",
    "#df.select(\"CabinNum\",\"Cabin\").distinct().collect()\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Length of name (One feature can be simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transf_new(r): # new transform\n",
    "    return LabeledPoint(\n",
    "        int(r.Survived),\n",
    "        [\n",
    "        #old features\n",
    "            int(r.Pclass),\n",
    "            r.Sex == 'male',\n",
    "            float(r.Fare),\n",
    "            int(r.SibSp),\n",
    "            int(r.Parch),\n",
    "            parse_age(r.Age),   \n",
    "        ] \n",
    "        + list(r.EmbarkedVec.toArray()) +\n",
    "        # new features\n",
    "        [\n",
    "            int(r.TicketNumber),\n",
    "            int(r.CabinNum),\n",
    "            len(r.Name),\n",
    "        ]\n",
    "        + list(r.NamePrefixVec.toArray())\n",
    "        + list(r.CabinLetterVec.toArray())\n",
    "    )\n",
    "\n",
    "def transf(r): # old transform\n",
    "    return LabeledPoint(\n",
    "        int(r.Survived),\n",
    "        [\n",
    "            int(r.Pclass),\n",
    "            r.Sex == 'male',\n",
    "            float(r.Fare),\n",
    "            int(r.SibSp),\n",
    "            int(r.Parch),\n",
    "            parse_age(r.Age),\n",
    "        ] + list(r.EmbarkedVec.toArray())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Calculate f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def acc(clf, test):\n",
    "    values = test.map(lambda x: x.features)\n",
    "    yhat = clf.predict(values)\n",
    "    y = test.map(lambda x: x.label)\n",
    "    comp = yhat.zip(y)\n",
    "    errors = comp.map(lambda x: abs(x[0]-x[1]))\n",
    "    return 1-errors.sum()/errors.count()\n",
    "\n",
    "def recall(clf, test):\n",
    "    values = test.map(lambda x: x.features)\n",
    "    yhat = clf.predict(values)\n",
    "    \n",
    "    y = test.map(lambda x: x.label)\n",
    "    comp = yhat.zip(y)\n",
    "    true_pos = comp.map(lambda x: float(x[0]==x[1] and x[0] == 1.0) ).sum()\n",
    "    true_cond_pos = y.sum()\n",
    "    return true_pos/true_cond_pos\n",
    "\n",
    "def precision(clf, test):\n",
    "    values = test.map(lambda x: x.features)\n",
    "    yhat = clf.predict(values)\n",
    "    \n",
    "    y = test.map(lambda x: x.label)\n",
    "    comp = yhat.zip(y)\n",
    "    true_pos = comp.map(lambda x: float(x[0]==x[1] and x[0] == 1.0) ).sum()\n",
    "    pred_cond_pos = yhat.sum()\n",
    "    \n",
    "    return true_pos/pred_cond_pos\n",
    "\n",
    "def f1_score(clf, test):\n",
    "    #print precision(clf, test), recall(clf,test)\n",
    "    return 2.0/(1.0/precision(clf, test)+1.0/recall(clf, test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = df.rdd.map(transf_new)\n",
    "train, test = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rustem/Programs/spark-2.2.0-bin-hadoop2.7//python/pyspark/mllib/classification.py:313: UserWarning: Deprecated in 2.0.0. Use ml.classification.LogisticRegression or LogisticRegressionWithLBFGS.\n",
      "  \"Deprecated in 2.0.0. Use ml.classification.LogisticRegression or \"\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.mllib.classification import LogisticRegressionModel,LogisticRegressionWithSGD\n",
    "from pyspark.mllib.classification import SVMModel, SVMWithSGD\n",
    "from pyspark.mllib.tree import GradientBoostedTrees, GradientBoostedTreesModel\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "\n",
    "rfc = RandomForest.trainClassifier(train, numClasses=2,\n",
    "                             categoricalFeaturesInfo={},\n",
    "                            numTrees=100)\n",
    "lr = LogisticRegressionWithSGD.train(train)\n",
    "dt = DecisionTree.trainClassifier(train, numClasses=2,categoricalFeaturesInfo={})\n",
    "gb = GradientBoostedTrees.trainClassifier(train, categoricalFeaturesInfo={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest       : f1 - 0.758620689655 \t acc - 0.800711743772\n",
      "Logistic Regression : f1 - 0.580808080808 \t acc - 0.409252669039\n",
      "Decision Tree       : f1 - 0.73786407767 \t acc - 0.807829181495\n",
      "Gradient Boosting   : f1 - 0.740088105727 \t acc - 0.790035587189\n"
     ]
    }
   ],
   "source": [
    "print 'Random Forest       : f1 - {0} \\t acc - {1}'.format(f1_score(rfc, test), acc(rfc,test))\n",
    "print 'Logistic Regression : f1 - {0} \\t acc - {1}'.format(f1_score(lr, test), acc(lr,test))\n",
    "print 'Decision Tree       : f1 - {0} \\t acc - {1}'.format(f1_score(dt, test), acc(dt,test))\n",
    "print 'Gradient Boosting   : f1 - {0} \\t acc - {1}'.format(f1_score(gb, test), acc(gb,test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
